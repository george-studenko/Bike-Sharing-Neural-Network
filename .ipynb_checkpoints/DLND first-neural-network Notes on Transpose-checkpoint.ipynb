{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation on DLND first-neural-network issues related to transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Nahua Kang\n",
    "**Source**: This file was downloaded from the repository of Nahua Kang [click here](https://github.com/nahuakang/deep-learning/blob/9e38ec1353a129d508202c1db7596964c0ec6de7/DLND-Projects/first-neural-network/DLND%20first-neural-network%20Notes%20on%20Transpose.ipynb) adding it to this repo to find it easier in the future, all credits go to Nahua Kang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since many of us, including myself, have had mistakes with the mismatching matrices' dimensions when calculating delta_weights_i_h and delta_weights_h_o, I decide to make a debugging process full of print() to explain why we made our mistakes and hopefully the insights will help us in the future when we design neural networks with multiple output elements.\n",
    "\n",
    "The notebook is separated in 4 sections:\n",
    "1. Data Preparation (feel free to skip)\n",
    "2. Explanation on issues related to transpose in train method of NeuralNetwork class\n",
    "3. Why we must use x[:, None] instead of x.T for transpose in Project 1?\n",
    "4. Minor detail in the update of delta_weights for hidden and output layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is literally the same as in our project 1 jupyter notebook. Feel free to skip to the next subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'Bike-Sharing-Dataset/hour.csv'\n",
    "\n",
    "rides = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\n",
    "for each in dummy_fields:\n",
    "    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n",
    "    rides = pd.concat([rides, dummies], axis=1)\n",
    "\n",
    "fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', \n",
    "                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']\n",
    "data = rides.drop(fields_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n",
    "# Store scalings in a dictionary so we can convert back later\n",
    "scaled_features = {}\n",
    "for each in quant_features:\n",
    "    mean, std = data[each].mean(), data[each].std()\n",
    "    scaled_features[each] = [mean, std]\n",
    "    data.loc[:, each] = (data[each] - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save data for approximately the last 21 days \n",
    "test_data = data[-21*24:]\n",
    "\n",
    "# Now remove the test data from the data set \n",
    "data = data[:-21*24]\n",
    "\n",
    "# Separate the data into features and targets\n",
    "target_fields = ['cnt', 'casual', 'registered']\n",
    "features, targets = data.drop(target_fields, axis=1), data[target_fields]\n",
    "test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hold out the last 60 days or so of the remaining data as a validation set\n",
    "train_features, train_targets = features[:-60*24], targets[:-60*24]\n",
    "val_features, val_targets = features[-60*24:], targets[-60*24:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explanation of issues related to transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, I will be running the train method under NeuralNetwork class for one sample of data. Therefore, I will not write down the complete class but simply the one-time for loop inside the train method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writing the hidden layer activation function\n",
    "sigmoid = lambda x: 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assigning the number to input_nodes, hidden_nodes, and output_nodes\n",
    "# in accordance with the dataset features, the desired hidden node number and the desired output number\n",
    "input_nodes, hidden_nodes = train_features.shape[1], 50\n",
    "output_nodes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims of weights for hidden and output layers: (56, 50) (50, 1)\n",
      "Dims of deltas for hidden and output layers: (56, 50) (50, 1)\n"
     ]
    }
   ],
   "source": [
    "# Declare the weights and delta_weights accordingly, just like in Project 1\n",
    "weights_input_to_hidden = np.random.normal(0.0, input_nodes**-0.5, (input_nodes, hidden_nodes));\n",
    "weights_hidden_to_output = np.random.normal(0.0, hidden_nodes**-0.5, (hidden_nodes, output_nodes));\n",
    "delta_weights_i_h = np.zeros(weights_input_to_hidden.shape)\n",
    "delta_weights_h_o = np.zeros(weights_hidden_to_output.shape)\n",
    "\n",
    "# Check the dimensions of the weights and delta_weights in both the hidden and output layers\n",
    "print(\"Dims of weights for hidden and output layers:\" , weights_input_to_hidden.shape, weights_hidden_to_output.shape)\n",
    "print(\"Dims of deltas for hidden and output layers:\", delta_weights_i_h.shape, delta_weights_h_o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the part below, since the debugging prints are messy in the for-loop, please read through the printing in the output and refer back to the code. My code is rather verbose but I hope you can understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim means dimension. Dot means dot product.\n",
      "\n",
      "X should be dim (1, 56), so basically 1 row with 56 features...\n",
      "But in Numpy, X is actually of dim (56,)\n",
      "By matrix dot product, matrix A of (n, p) dot product matrix B of (p, m) results in matrix C of (n, m)\n",
      "Therefore, X (1, 56) dot weights (56, 50) result should be in dim (1, 50)\n",
      "But the result in Numpy is actually of dim (50,)\n",
      "\n",
      "hidden_outputs (1, 50) dot weights (50, 1) should result in an output of (1, 1)...\n",
      "But the result in Numpy is actually of dim (1,)\n",
      "\n",
      "The dim of output_error_term in Numpy should be (1, 1), but is in fact: (1,)\n",
      "The dim of weights_hidden_to_output is (50, 1)\n",
      "The dim of hidden_error_term in Numpy should be (50, 1), but is in fact: (50,)\n",
      "\n",
      "Remember again that the dims of delta_weights_i_h, delta_weights_h_o are: (56, 50) (50, 1)\n",
      "Remember again that the dims of inputs X and final_outputs should be (1, 56), (1, 1)\n",
      "So delta_weights_i_h's dim should be the same as the dot product between some form of X and hidden_error_term\n",
      "So delta_weights_h_o's dim should be the same as the dot product between some form of hidden_outputs and output_error_term\n",
      "\n",
      "(X' means X transpose) X' of dim(56, 1) dot hidden_error_term of dim(1, 50) would result in dim(56, 50)\n",
      "This dim(56, 50) is aligned with delta_weights_i_h's dimension.\n",
      "(hidden_outputs' means hidden_outputs transpose) The dim of hidden_outputs is (1, 50)\n",
      "The same applies to delta_weights_h_o: hidden_outputs' of dim(50, 1) dot output_error_term of dim(1, 1) would result in dim(50, 1)\n",
      "This dim(50, 1) is aligned with delta_weights_h_o's dimension.\n",
      "\n",
      "Since in Numpy output_error_term does not have dim(1, 1) but (1,)\n",
      "Since in Numpy hidden_error_term does not have dim(50, 1) but (50,)\n",
      "We can instead perform hidden_error_term * X' or more precisely 'hidden_error_term * X[:, None]'\n",
      "Because hidden_error_term * X[:, None] has dim (56, 50) , just what we wanted from above\n",
      "The same applies to delta_weights_h_o. Perform output_error_term * hidden_outputs[:, None] achieves the same\n",
      "Because output_error_term * hidden_outputs[:, None] has dim (50, 1) , just what we wanted from above\n",
      "\n",
      "SO WHY CAN'T WE USE THE METHOD X.T FOR TRANSPOSE BUT MUST USE X[:, None]?\n"
     ]
    }
   ],
   "source": [
    "# Running the for-loop under NeuralNetwork class' train method once to demonstrate the dimension changes:\n",
    "for X, y in zip(train_features.values[:1], train_targets[\"cnt\"].values[:1]):\n",
    "    print(\"Dim means dimension. Dot means dot product.\")\n",
    "    \n",
    "    print()\n",
    "    print(\"X should be dim (1, 56), so basically 1 row with 56 features...\")\n",
    "    print(\"But in Numpy, X is actually of dim\", X.shape)\n",
    "    print(\"By matrix dot product, matrix A of (n, p) dot product matrix B of (p, m) results in matrix C of (n, m)\")\n",
    "    print(\"Therefore, X (1, 56) dot weights (56, 50) result should be in dim (1, 50)\")\n",
    "    hidden_inputs = np.dot(X, weights_input_to_hidden)\n",
    "    print(\"But the result in Numpy is actually of dim\", hidden_inputs.shape)\n",
    "    hidden_outputs = sigmoid(hidden_inputs)\n",
    "    \n",
    "    print()\n",
    "    final_inputs = np.dot(hidden_outputs, weights_hidden_to_output)\n",
    "    final_outputs = final_inputs # Assuming activation function for output layer is f(x) = x\n",
    "    print(\"hidden_outputs (1, 50) dot weights (50, 1) should result in an output of (1, 1)...\")\n",
    "    print(\"But the result in Numpy is actually of dim\", final_outputs.shape)\n",
    "    \n",
    "    print()\n",
    "    output_error = y - final_outputs\n",
    "    output_error_term = output_error * 1 # Since the derivative of output activation f(x) = dx/dx = 1\n",
    "    print(\"The dim of output_error_term in Numpy should be (1, 1), but is in fact:\", output_error_term.shape)\n",
    "    hidden_error = np.dot(weights_hidden_to_output, output_error_term)\n",
    "    hidden_error_term = hidden_error * hidden_outputs * (1 - hidden_outputs)\n",
    "    print(\"The dim of weights_hidden_to_output is\", weights_hidden_to_output.shape)\n",
    "    print(\"The dim of hidden_error_term in Numpy should be (50, 1), but is in fact:\", hidden_error_term.shape)\n",
    "    \n",
    "    print()\n",
    "    print(\"Remember again that the dims of delta_weights_i_h, delta_weights_h_o are:\", delta_weights_i_h.shape, delta_weights_h_o.shape)\n",
    "    print(\"Remember again that the dims of inputs X and final_outputs should be (1, 56), (1, 1)\")\n",
    "    print(\"So delta_weights_i_h's dim should be the same as the dot product between some form of X and hidden_error_term\")\n",
    "    print(\"So delta_weights_h_o's dim should be the same as the dot product between some form of hidden_outputs and output_error_term\")\n",
    "    \n",
    "    print()\n",
    "    print(\"(X' means X transpose) X' of dim(56, 1) dot hidden_error_term of dim(1, 50) would result in dim(56, 50)\")\n",
    "    print(\"This dim(56, 50) is aligned with delta_weights_i_h's dimension.\")\n",
    "    print(\"(hidden_outputs' means hidden_outputs transpose) The dim of hidden_outputs is (1, 50)\")\n",
    "    print(\"The same applies to delta_weights_h_o: hidden_outputs' of dim(50, 1) dot output_error_term of dim(1, 1) would result in dim(50, 1)\")\n",
    "    print(\"This dim(50, 1) is aligned with delta_weights_h_o's dimension.\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Since in Numpy output_error_term does not have dim(1, 1) but\", output_error_term.shape)\n",
    "    print(\"Since in Numpy hidden_error_term does not have dim(50, 1) but\", hidden_error_term.shape)\n",
    "    print(\"We can instead perform hidden_error_term * X' or more precisely 'hidden_error_term * X[:, None]'\")\n",
    "    delta_weights_i_h += hidden_error_term * X[:, None]\n",
    "    print(\"Because hidden_error_term * X[:, None] has dim\", (hidden_error_term * X[:, None]).shape, \", just what we wanted from above\")\n",
    "    delta_weights_h_o += output_error_term * hidden_outputs[:, None]\n",
    "    print(\"The same applies to delta_weights_h_o. Perform output_error_term * hidden_outputs[:, None] achieves the same\")\n",
    "    print(\"Because output_error_term * hidden_outputs[:, None] has dim\", (output_error_term * hidden_outputs[:, None]).shape, \", just what we wanted from above\")\n",
    "    \n",
    "    print()\n",
    "    print(\"SO WHY CAN'T WE USE THE METHOD X.T FOR TRANSPOSE BUT MUST USE X[:, None]?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Why we must use X[:, None] instead of X.T?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because in Numpy, when we have 3 inputs from the dataset, it does not come in the dimension of (1, 3) like how we describe the matrices of the neural network on paper. Instead, the inputs come in the Numpy shape of (3,). Look at the numpy array x as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example\n",
    "x = np.array([1, 2, 3])\n",
    "# On paper, x should have dim(1, 3), but in Numpy it has dim(3,)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to take the transpose of x using the .T method, we would assume that the resulting x' has dimension (3, 1) as opposed to (1, 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah! Despite using the transpose method, x.T still has dimension (3,). This is not the best transpose method to use if we need to conduct * or np.dot on two matrices, like the ones we did in the train method of NeuralNetwork class!\n",
    "\n",
    "Using x[:, None], however, will solve this issue! This is why we prefer using x[:, None] for this course, I guess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, None].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Minor detail in the update of delta_weights in hidden and output layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might also wonder why we used:\n",
    "    delta_weights_i_h += hidden_error_term \\* X[:, None]\n",
    "    delta_weights_h_o += output_error_term \\* hidden_outputs[:, None]\n",
    "for updating the delta_weights. There's no np.dot() involved here! Why?\n",
    "\n",
    "Well, since the dimensions of hidden_error_term and output_error_term are not appropriate for conducting np.dot(), using the simple matrices multiplication with asterisk \\* will perform the same job for our implementation. See the example below for what the asterisk \\* does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 4, 6],\n",
       "       [3, 6, 9]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1, 2, 3]) * np.array([1, 2, 3])[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the same as the one below, in which we specifically designed it so the first array has dimension (3, 1) and the second array has dimension (1, 3) instead of (3,). This is how we would update the delta_weights on paper, but not how we would perform in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 4, 6],\n",
       "       [3, 6, 9]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.array([1,2,3])[:, None], np.array([[1,2,3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope this notebook has helped you! Feel free to give me feedback in Slack chat or at kangnahua(at)gmail.com! Best luck to your learning :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
